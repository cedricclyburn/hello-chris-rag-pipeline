{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ad2cc4e-31ec-4648-b0fe-6632f2bdbc36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Extending the capabilities of our model\n",
    "\n",
    "An LLM is a very capable tool, but only to the extent of the knowledge or information it has been trained on. After all, you only know what you know, right? But what if you need to ask a question that is not in the training data? Or what if you need to ask a question that is not in the training data, but is related to it?\n",
    "\n",
    "There are different ways to solve this problem, depending on the resources you have and the time or money you can spend on it. Here are a few options:\n",
    "\n",
    "- Fully retrain the model to include the information you need. For an LLM, it's only possible for a handful of companies in the world that can afford literally thousands of GPUs running for weeks.\n",
    "- Fine-tune the model with this new information. This requires way less resources, and can usually be done in a few hours or minutes (depending on the size of the model). However as it does not fully retrain the model, the new information may not be completely integrated in the answers. Fine-tuning excels at giving a better understanding of a specific context or vocabulary, a little bit less on injecting new knowledge. Plus you have to retrain and redeploy the model anyway any time you want to add more information.\n",
    "- Put this new information in a database and have the parts relevant to the query retrieved and added to this query as a context before sending it to the LLM. This technique is called **Retrieval Augmented Generation, or RAG**. It is interesting as you don't have to retrain or fine-tune the model to benefit of this new knowledge, that you can easily update at any time.\n",
    "\n",
    "We have already prepared a Vector Database using [Milvus](https://milvus.io/), where we have stored (in the form of [Embeddings](https://www.ibm.com/topics/embedding)) the content of the [California Driver's Handbook](https://www.dmv.ca.gov/portal/handbook/california-driver-handbook/).\n",
    "\n",
    "In this Notebook, we are going to use RAG to **make some queries about a Claim** and see how this new knowledge can help without having to modify our LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e2b81-0e10-4390-a7b8-5ddfda53a3e3",
   "metadata": {},
   "source": [
    "### Requirements and Imports\n",
    "\n",
    "If you have selected the right workbench image to launch as per the Lab's instructions, you should already have all the needed libraries. If not uncomment the first line in the next cell to install all the right packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d61c595d-967e-47de-a598-02b5d1ccec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line only if you have not selected the right workbench image, or are using this notebook outside of the workshop environment.\n",
    "# !pip install --no-cache-dir --no-dependencies --disable-pip-version-check -r requirements.txt\n",
    "\n",
    "import json\n",
    "import transformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.llms import VLLMOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Milvus # Use the standard Milvus vector store\n",
    "\n",
    "# Turn off warnings when downloading the embedding model\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c428fbad-2345-4536-b687-72416d6b9b15",
   "metadata": {},
   "source": [
    "### Langchain elements\n",
    "\n",
    "Again, we are going to use Langchain to define our task pipeline.\n",
    "\n",
    "First, the **LLM** where we will send our queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77f95a70-89fb-4e21-a51c-24e862b7953e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LLM Inference Server URL\n",
    "inference_server_url = \"http://granite-3-1-8b-instruct-predictor.shared-llm.svc.cluster.local:8080\"\n",
    "\n",
    "# LLM definition\n",
    "llm = VLLMOpenAI(           # We are using the vLLM OpenAI-compatible API client. But the Model is running on OpenShift AI, not OpenAI.\n",
    "    openai_api_key=\"EMPTY\",   # And that is why we don't need an OpenAI key for this.\n",
    "    openai_api_base= f\"{inference_server_url}/v1\",\n",
    "    model_name=\"granite-3-1-8b-instruct\",\n",
    "    top_p=0.92,\n",
    "    temperature=0.01,\n",
    "    max_tokens=512,\n",
    "    presence_penalty=1.03,\n",
    "    streaming=True,\n",
    "    callbacks=[StreamingStdOutCallbackHandler()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13907-14f1-4995-9756-8778c19a2101",
   "metadata": {},
   "source": [
    "Then the connection to the **vector database** where we have prepared and stored the California Driver Handbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f849c1a0-7fe5-425f-853d-6a9e67a38971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model loaded.\n",
      "Connecting to Milvus at: vectordb-milvus:19530\n",
      "Successfully connected to Milvus collection 'servicenow_incidents'.\n",
      "Retriever created successfully.\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Milvus\n",
    "\n",
    "# 1. Define the embedding model (must match the ingestion pipeline)\n",
    "print(\"Loading embedding model...\")\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"all-MiniLM-L6-v2\",\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    show_progress=False,\n",
    ")\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# 2. Define connection arguments\n",
    "connection_args = {\n",
    "    \"host\": \"vectordb-milvus\", # The Kubernetes service name for Milvus\n",
    "    \"port\": \"19530\"\n",
    "}\n",
    "print(f\"Connecting to Milvus at: {connection_args['host']}:{connection_args['port']}\")\n",
    "\n",
    "# 3. Connect to the Milvus vector store, specifying the correct text and vector fields\n",
    "vector_db = Milvus(\n",
    "    embedding_function=embeddings,\n",
    "    connection_args=connection_args,\n",
    "    collection_name=\"servicenow_incidents\",\n",
    "    vector_field=\"embedding\",      # Specify the name of your vector field\n",
    "    text_field=\"resolution_notes\"  # <-- THIS IS THE FIX: Tell LangChain to use this field for page_content\n",
    ")\n",
    "print(\"Successfully connected to Milvus collection 'servicenow_incidents'.\")\n",
    "\n",
    "\n",
    "# 4. Create a retriever to search for relevant documents\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "print(\"Retriever created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b950bc-4d73-49e5-a35b-083a784edd50",
   "metadata": {},
   "source": [
    "We will now define the **template** to use to make our query. Note that this template now contains a **References** section. That's were the documents returned from the vector database will be injected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849fbd67-220c-4a02-8e4e-7e0d1aa91588",
   "metadata": {},
   "source": [
    "We are now ready to query the model!\n",
    "\n",
    "In the `claims` folder we have JSON files with examples of claims that could be received. We are going to read the first claim and ask a question related to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4cde40-3571-4e3c-9b05-78389765c98f",
   "metadata": {},
   "source": [
    "### First test, no additional knowledge\n",
    "\n",
    "Let's start with a first query about the claim, but without help from our vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f714d-c6e7-4220-a16b-fc65dbae91fb",
   "metadata": {},
   "source": [
    "We can see that the answer is valid. Here the model is using its general understanding of traffic regulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4e9a93-9b81-424a-96a9-f447e417c8c1",
   "metadata": {},
   "source": [
    "### Second test, with added knowledge\n",
    "\n",
    "We will use the same prompt and query, but this time the model will have access to some references from the California's Driver Handbook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dac009d5-d558-4258-9735-4fb0de46c309",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing RAG chain with query: 'Give me information on INC001004'\n",
      "INC001004 refers to an incident where the primary network interface card (NIC) of the main email server failed. The issue was resolved by rerouting traffic to the secondary NIC and ordering a replacement primary NIC. Once installed and configured, full redundancy will be restored. Users confirmed that email services were restored after the failover. No issues were found with the VPN concentrator or client software.\n",
      "--- Answer from LLM ---\n",
      "\n",
      "INC001004 refers to an incident where the primary network interface card (NIC) of the main email server failed. The issue was resolved by rerouting traffic to the secondary NIC and ordering a replacement primary NIC. Once installed and configured, full redundancy will be restored. Users confirmed that email services were restored after the failover. No issues were found with the VPN concentrator or client software.\n",
      "\n",
      "--- Sources Retrieved from Milvus ---\n",
      "\n",
      "  - Incident: INC001005\n",
      "    Description: Printer 'PRN-FIN-01' jamming frequently\n",
      "--------------------\n",
      "  - Incident: INC001001\n",
      "    Description: Email server unresponsive\n",
      "--------------------\n",
      "  - Incident: INC001003\n",
      "    Description: VPN connection drops frequently\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "# The 'llm' and 'retriever' objects should already be defined from the previous cells.\n",
    "\n",
    "# 1. Define a prompt template suitable for a general Q&A over the retrieved documents.\n",
    "#    This template tells the LLM how to use the context from Milvus to answer the question.\n",
    "prompt_template_str = \"\"\"\n",
    "<|system|>\n",
    "You are a helpful, respectful and honest assistant named \"Parasol Assistant\".\n",
    "You will be given context from past incident tickets and a question.\n",
    "Your answer should be based only on the provided context.\n",
    "If the context does not contain the answer, say that you don't have enough information.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template_str, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# 2. Create the RAG chain with the new prompt.\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": prompt},\n",
    "    return_source_documents=True,\n",
    ")\n",
    "\n",
    "# 3. Define the query and invoke the chain.\n",
    "query = \"Give me information on INC001004\"\n",
    "print(f\"Executing RAG chain with query: '{query}'\")\n",
    "\n",
    "resp = rag_chain.invoke({\"query\": query})\n",
    "\n",
    "# 4. Print the results.\n",
    "print(\"\\n--- Answer from LLM ---\\n\")\n",
    "print(resp[\"result\"])\n",
    "print(\"\\n--- Sources Retrieved from Milvus ---\\n\")\n",
    "for doc in resp[\"source_documents\"]:\n",
    "    print(f\"  - Incident: {doc.metadata.get('incident_pk', 'N/A')}\")\n",
    "    print(f\"    Description: {doc.metadata.get('short_description', 'N/A')}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5659f0-a27f-4b9e-8dd1-e05f37671c8f",
   "metadata": {},
   "source": [
    "That is pretty neat! Now the model refers more precisely to the rules that must be observed.\n",
    "\n",
    "But where did we get this information from? We can look into the sources associated with the answers from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e424c52-9a1a-4425-a105-4e0744ec0da6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_sources(input_list):\n",
    "    sources = \"\"\n",
    "    if len(input_list) != 0:\n",
    "        sources += input_list[0].metadata[\"metadata\"][\"source\"] + ', page: ' + str(input_list[0].metadata[\"metadata\"][\"page\"])\n",
    "        page_list = [input_list[0].metadata[\"metadata\"][\"page\"]]\n",
    "        for item in input_list:\n",
    "            if item.metadata[\"metadata\"][\"page\"] not in page_list: # Avoid duplicates\n",
    "                page_list.append(item.metadata[\"metadata\"][\"page\"])\n",
    "                sources += ', ' + str(item.metadata[\"metadata\"][\"page\"])\n",
    "    return sources\n",
    "\n",
    "\n",
    "results = format_sources(resp['source_documents'])\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8cd32-0bdb-484d-a8bd-fb108ce2f131",
   "metadata": {},
   "source": [
    "That's it! We now know how to complement our LLM with some external knowledge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
