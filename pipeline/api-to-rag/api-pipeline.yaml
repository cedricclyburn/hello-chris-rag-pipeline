mport kfp
from kfp import dsl
from kfp.dsl import InputPath, OutputPath

# Define the base image for the components. Using a standard Python image.
BASE_IMAGE = 'python:3.9-slim'

# Define the image for the docling component, as specified in your example.
DOCLING_IMAGE = 'quay.io/bbrowning/docling-kfp:v2.25.0'

# Component to fetch data from the mock ServiceNow API
@dsl.component(
    base_image=BASE_IMAGE,
    packages_to_install=['requests==2.28.1']
)
def fetch_data_from_api(
    api_endpoint: str,
    incidents_data: OutputPath("json")
):
    """Fetches closed-incident data from a ServiceNow-like API."""
    import requests
    import json

    print(f"Fetching data from endpoint: {api_endpoint}")
    
    # Parameters to fetch all closed incidents
    params = {'state': 'closed', 'limit': 200} # Increased limit to get all mock data
    
    try:
        response = requests.get(api_endpoint, params=params)
        response.raise_for_status()  # Raises an error for bad responses
        
        data = response.json()
        
        with open(incidents_data, 'w') as f:
            json.dump(data, f)
            
        print(f"Successfully fetched {len(data.get('result', []))} incidents and saved to {incidents_data}")
        
    except requests.exceptions.RequestException as e:
        print(f"Error fetching data from API: {e}")
        raise

# Component to parse data and ingest it into Milvus
@dsl.component(
    base_image=BASE_IMAGE,
    packages_to_install=['pymilvus==2.3.1', 'sentence-transformers==2.2.2', 'torch==1.13.1']
)
def ingest_incidents_to_milvus(
    incidents_data: InputPath("json"),
    milvus_host: str,
    milvus_port: str,
    collection_name: str = "servicenow_incidents",
):
    """Parses incident data and ingests it into a Milvus vector database."""
    import json
    from pymilvus import connections, utility, FieldSchema, CollectionSchema, DataType, Collection
    from sentence_transformers import SentenceTransformer

    # 1. Connect to Milvus
    print(f"Connecting to Milvus at {milvus_host}:{milvus_port}")
    try:
        connections.connect("default", host=milvus_host, port=milvus_port)
        print("Successfully connected to Milvus.")
    except Exception as e:
        print(f"Failed to connect to Milvus: {e}")
        raise

    # 2. Define the collection schema
    # We will create vectors from a combination of short_description and resolution_notes
    embedding_dim = 384  # Based on the 'all-MiniLM-L6-v2' model
    
    fields = [
        FieldSchema(name="incident_pk", dtype=DataType.VARCHAR, is_primary=True, max_length=20),
        FieldSchema(name="short_description", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="resolution_notes", dtype=DataType.VARCHAR, max_length=4096),
        FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim)
    ]
    
    schema = CollectionSchema(fields, "ServiceNow Incidents Collection")
    
    # 3. Create the collection if it doesn't exist
    if utility.has_collection(collection_name):
        print(f"Collection '{collection_name}' already exists. Dropping for fresh load.")
        utility.drop_collection(collection_name)
        
    print(f"Creating collection: {collection_name}")
    collection = Collection(collection_name, schema)

    # 4. Load the data and generate embeddings
    print("Loading incident data from artifact...")
    with open(incidents_data, 'r') as f:
        data = json.load(f)
    
    incidents = data.get('result', [])
    if not incidents:
        print("No incidents found in the data. Exiting.")
        return

    # Load a pre-trained model for creating embeddings
    print("Loading sentence-transformer model 'all-MiniLM-L6-v2'...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Prepare data for insertion
    incident_pks = []
    short_descriptions = []
    resolution_notes_list = []
    embeddings = []
    
    print(f"Preparing and embedding {len(incidents)} incidents...")
    for inc in incidents:
        # We only want to ingest incidents with resolution notes
        if inc.get('resolution_notes'):
            incident_pks.append(inc['number'])
            short_descriptions.append(inc.get('short_description', ''))
            
            resolution_note = inc['resolution_notes']
            resolution_notes_list.append(resolution_note)
            
            # Combine fields for a richer embedding
            text_to_embed = f"Title: {inc.get('short_description', '')}\nResolution: {resolution_note}"
            embeddings.append(model.encode(text_to_embed))

    if not incident_pks:
        print("No incidents with resolution notes found to ingest. Exiting.")
        return
        
    # 5. Insert data into Milvus
    entities = [
        incident_pks,
        short_descriptions,
        resolution_notes_list,
        embeddings
    ]
    
    print(f"Inserting {len(incident_pks)} entities into Milvus...")
    insert_result = collection.insert(entities)
    collection.flush() # Ensure data is written to disk
    
    print(f"Successfully inserted entities. Mutation result: {insert_result}")
    
    # 6. Create an index for the collection for efficient searching
    index_params = {
        "metric_type": "L2",
        "index_type": "IVF_FLAT",
        "params": {"nlist": 1024}
    }
    print(f"Creating index with params: {index_params}")
    collection.create_index(
        field_name="embedding", 
        index_params=index_params
    )
    collection.load()
    print("Index created and collection loaded into memory.")

# The pipeline definition that orchestrates the components
@dsl.pipeline(
    name="ServiceNow to Milvus Ingestion Pipeline",
    description="Fetches incident data from a ServiceNow-like API and ingests it into a Milvus vector database."
)
def servicenow_to_milvus_pipeline(
    api_endpoint: str,
    milvus_host: str,
    milvus_port: str,
    collection_name: str = "servicenow_incidents"
):
    # Task 1: Fetch data from the API
    fetch_task = fetch_data_from_api(
        api_endpoint=api_endpoint
    )
    fetch_task.set_display_name("Fetch ServiceNow Incidents")

    # Task 2: Ingest data into Milvus
    ingest_task = ingest_incidents_to_milvus(
        incidents_data=fetch_task.outputs["incidents_data"],
        milvus_host=milvus_host,
        milvus_port=milvus_port,
        collection_name=collection_name
    )
    ingest_task.set_display_name("Ingest Incidents to Milvus")

if __name__ == '__main__':
    # This section is for compiling the pipeline.
    # It will not be executed when the pipeline runs on KFP, but by you on your local machine
    # to create the pipeline package.
    from kfp.compiler import Compiler
    Compiler().compile(
        pipeline_func=servicenow_to_milvus_pipeline,
        package_path='servicenow_to_milvus_pipeline.yaml'
    )
    print("Pipeline compiled to servicenow_to_milvus_pipeline.yaml")