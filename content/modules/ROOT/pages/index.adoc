= Building an Enterprise RAG System with OpenShift AI
include::_attributes.adoc[]

== Workshop Overview

Welcome to the {company-name} RAG Workshop! This {ic-lab} will illustrate how to combine event-driven serverless applications, data science pipelines, and a vector database to solve a real-world enterprise challenge using Large Language Models (LLMs).

The scenario for this lab is that we all work for "{company-name}", a large company looking to leverage AI to improve its IT support operations. Our goal is to build a prototype Retrieval-Augmented Generation (RAG) system that taps into the vast knowledge base of closed ServiceNow tickets, making it instantly searchable for support engineers through a conversational AI assistant.

== Disclaimer

This {ic-lab} is an example of what a customer could build using {rhoai}. {rhoai} itself does not include a ServiceNow integration or a vector database out-of-the-box.

This {ic-lab} makes use of a Large Language Model (LLM), the `docling` document parsing library, and the Milvus vector database. These models and third-party tools are not included in the {rhoai} product. They are provided as a convenience for this {ic-lab}.

The quality of the models and the architecture are suitable for a prototype. Choosing the right components for a production environment is a complex task that requires significant experimentation and tuning. This {ic-lab} does not cover production-level architecture design.

== Timetable

This is a tentative timetable for the materials that will be presented.

[width="90%",cols="3,^2,^2,10",options="header"]
|=========================================================
| Name |Duration |Type |Description

|The Parasol Company Scenario |10 | Presentation
a|- We describe the business problem: knowledge is trapped in unstructured ServiceNow tickets.
- We propose a solution using a RAG architecture to power an intelligent assistant.
- We discuss the benefits of using OpenShift AI, Knative, and Milvus for this solution.

|Demo: Event-Driven PDF Pipeline |10 | Demo
a|- The instructor will demonstrate a real-time data ingestion pipeline.
- An upload of a PDF to an S3 bucket will automatically trigger a Kubeflow Pipeline via Knative Eventing and Kafka.
- This showcases the event-driven capabilities of the platform.

|Lab: Building the API-to-RAG Pipeline |60 | Hands-On
a|- **Lab Setup:** Attendees log in and explore their pre-created OpenShift AI project.
- **Deploy & Verify Mock API:** Attendees deploy a mock ServiceNow API using ArgoCD and test its endpoint.
- **Run KFP Pipeline:** Attendees import and run a Kubeflow Pipeline that fetches data from the API, generates embeddings, and populates the Milvus vector database.
- **Query the RAG System:** Attendees use a Jupyter Notebook to ask questions against the system and see the contextually relevant answers retrieved from Milvus.

|What's Next? |10 | Presentation
a|- Discuss how to improve the system's performance and intelligence using advanced techniques.
- Introduce InstructLab and the Retrieval-Augmented Fine-Tuning (RAFT) method as the next logical step to enhance the LLM's capabilities.

|=========================================================


== Contributing

If you are interested in contributing to this project, consult this GitHub Repo: https://github.com/cnuland/hello-chris-rag-pipeline[https://github.com/cnuland/hello-chris-rag-pipeline,window=_blank]